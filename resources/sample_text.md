# An AI Programming Itself?
Autonomous Agents, and some incredible applications

After GPT-4 was released, a very interesting capability was discovered: the model was able to recognise its own mistakes. If you asked ChatGPT to create a poem that rhymes in every verse it might have 70% of them correctly. But if you ask it if it did it well, it will tell you that it was wrong and give you a better version of the poem. You don‚Äôt even need to tell it that it was wrong or where it was wrong, the model itself can learn from its own mistakes. So people thought ‚ÄúOkay, so the model works better if I give some feedback to it. But it‚Äôs also able to find its own errors‚Ä¶ Wait a minute! üí° What if I make two models talk so one gives feedback to the other!‚Äù And AutoGPT (and others) was born.

Another important concept in the Intelligent Agent. That is a system that perceives its environment, takes actions autonomously to achieve goals and may improve its performance by learning or acquiring knowledge. In simple terms, a system that interacts and learns from its environment. The basic ChatGPT version we use daily can have great ideas, it knows all the steps to make a tasty raspberry pie, but if we ask it to execute them, it will tell us that - It is just a language model and blah blah blah.

ChatGPT Plus, the pro version of ChatGPT you get for the modic price of 20 dollars a month, has actually access to some tools. It cannot make a raspberry pie for you but it can use some other interesting tools like DALL-E, allowing it to create images, it has a Python console so it can execute code, make some calculations and even generate charts, and you can connect it to whatever service you want using an API (this is just a way to communicate different systems in the internet).

So, what happens when we mix both of these ideas? Multiple GPTs with access to different tools interacting between them? Well, the possibilities start to get really cool.

## Some Cool Agents
### Sims-like autonomous agents launching parties
In a very interesting paper called Generative Agents: Interactive Simulacra of Human Behavior, some researchers from Stanford and Google, made a bunch of agents interact with each other in a virtual world similar to the Sims to see how they behave. The agents were given the ability to walk in a virtual world, talk to other agents and interact with their environment. And they discovered that these agents started behaving a lot like us! One of them was ‚Äúprogrammed‚Äù with the desire to throw a Valentine‚Äôs Party and the invitation was spread among the other agents, they asked each other on dates and they all arrived at the party at the correct time and date.


### Voyager playing Minecraft
Another interesting agent is Voyager, an agent that learned to play Minecraft. It has access to some basic actions using an interface designed to move the character via code and it learned to execute more and more complex tasks building on top of the basic ones. It explored a big area of the map and it advanced in the use and crafting of tools.


### Devin designing complex Code
But that‚Äôs not all, two weeks ago (march 12) a research lab called Cognition AI introduced Devin, the first AI software engineer. It is an autonomous agent with access to the tools a developer uses daily: a shell, code editor and a browser. It has shown good results in long-term reasoning and planning, and it can plan and execute complex engineering tasks that require thousands of decisions. They show a bunch of video examples on their web page including one where Devin trained another AI model.

That particular job (training the model) is not the most impressive one in my opinion but it is the one with, in my opinion, the deepest implications. Imagine a really good programming system, maybe Devin or another similar agent, that can improve its own code, get more tools, add some features, and maybe even make itself smarter. If you think about it for a while, things can go out of control really quickly.

### Figure 01 Interacting with the world
Finally, another way AI agents can interact is, of course, a physical body -oh yeah, I‚Äôm talking about robots-. So OpenAI partnered with a robotic company called Figure and a week ago they released Figure 01. In this demo, this Robot uses ChatGPT to see, talk and see and uses its robotic body to interact with its environment, take an apple, pick up the trash, move the dishes, etc. I highly suggest you watch the video because it‚Äôs astonishing.

What else can I say? We are starting to live in the future, humanoid robots are starting to appear, space launches and telekinesis - Wait, what? - Oh yeah, Neuralink just streamed their first user moving a cursor with his mind. The next step is probably to connect this to some motor effector to move some mechanical arm or something, but I‚Äôll go deeper into this in another newsletter.

Have a nice week!
